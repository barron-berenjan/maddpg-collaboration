{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm\n",
    "\n",
    "<a href=\"https://arxiv.org/pdf/1706.02275.pdf\"> MADDPG </a> (Multi Agent Deep Determinstic Policy Gradients) extends <a href=\"https://arxiv.org/abs/1509.02971\">DDPG</a> by introducing the concept of __Centralized Training__ and __Decentralized Execution__ where policies use extra information from each other during training,  but are not reliant on this information during execution. In essence, as depicted below, the critic is\n",
    "augmented with extra information about the policies of other agents, while the actor only has access\n",
    "to local information. \n",
    "    \n",
    "\n",
    "<img src=\"./img/maddpg.png\" width=450, height=450>\n",
    "<center><a href=\"https://arxiv.org/pdf/1706.02275.pdf\">source</a> </center>\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "## Neural Network Architecture\n",
    "\n",
    "The network architecture and hyperparameters used for the agents are below:\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<center> Actor </center>\n",
    "\n",
    "| Layer | Input  | Output   |   \n",
    "|:-------|:--------|:----------|\n",
    "|FC1    |   24 (state space)  |  64       |   \n",
    "|FC2    |   64   |  64      |   \n",
    "|FC3    |   64     |   2 (action space)    |\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center> Critic </center>\n",
    "\n",
    "| Layer | Input  | Output   |   \n",
    "|:-------|:--------|:----------|\n",
    "|FC1    |   24 (state space)   |64|   \n",
    "|FC2    |   64 + 4 (action space) |64|   \n",
    "|FC3    |   64     |   1  (Q-value)  |\n",
    "\n",
    "<br>\n",
    "\n",
    "##  Parameters used for training :\n",
    "\n",
    "```python\n",
    "\n",
    "CONFIG = {\n",
    "    \"BUFFER_SIZE\": int(1e6),     # replay buffer size\n",
    "    \"BATCH_SIZE\": 512,           # minibatch size\n",
    "    \"GAMMA\": 0.95,               # discount factor\n",
    "    \"TAU\": 1e-2,                 # for soft update of target parameters\n",
    "    \"LR_ACTOR\": 1e-3,            # learning rate of the actor\n",
    "    \"LR_CRITIC\": 1e-3,           # learning rate of the critic\n",
    "    \"WEIGHT_DECAY\": 0,           # L2 weight decay\n",
    "    \"SIGMA\": 0.001,               # std of noise\n",
    "    \"CLIP_GRADS\": True,          # Whether to clip gradients\n",
    "    \"CLAMP_VALUE\": 0.5,          # Clip value\n",
    "    \"FC1\": 64,                   # First linear layer size\n",
    "    \"FC2\": 64,                   # Second linear layer size\n",
    "    \"WARMUP\": 0,                 # number of warmup steps\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "## Plot of Rewards\n",
    "\n",
    "Below is a plot of the agent's score during training. The agent is able to collect an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents) in ~1170 steps. In the code, the agent stops training as soon as it is able to achieve this score, but if we had left it to train longer then it would have achieved a higher score.\n",
    "\n",
    "<img src=\"img/tennis_maddpg.png\" width=450, height=450>\n",
    "\n",
    "\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "A few things that can be tried to improve model performance are:\n",
    "\n",
    "- Tuning of hyper-parameters:\n",
    "\n",
    "    - number of hidden cells\n",
    "    - number of hidden layers\n",
    "    - actor/ critic learning rates\n",
    "    - batch size\n",
    "    - noise regularization\n",
    "    \n",
    "- Implement Policy Ensembles as suggested in the original paper\n",
    "- Explore the idea of incorporating Priortized Experience Replay as suggested in this <a href=\"https://cardwing.github.io/files/RL_course_report.pdf\">paper </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nteract": {
   "version": "0.23.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
